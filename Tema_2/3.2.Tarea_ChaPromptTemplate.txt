Tarea: Mejora tu Chatbot con ChatPromptTemplate, GPT y OpenAI
üìã Objetivo

En esta tarea transformar√°s tu chatbot del tema anterior para usar ChatPromptTemplate en lugar de PromptTemplate. Descubrir√°s las 
ventajas de trabajar con templates dise√±ados espec√≠ficamente para modelos de chat y c√≥mo estructurar prompts de forma m√°s clara y eficiente.

‚ö†Ô∏è Importante: Esta tarea se centra en conceptos fundamentales de LangChain para aplicaciones conversacionales. Si encuentras 
dificultades, no te preocupes, el objetivo es que comprendas las diferencias entre ambos enfoques y sus casos de uso. La soluci√≥n 
completa estar√° disponible al final de este art√≠culo. ¬°Experimenta y aprende las mejores pr√°cticas!



üéØ Lo que aprender√°s

ChatPromptTemplate: Templates optimizados para modelos de chat

Estructura de mensajes: System, Human, AI messages en templates

Separaci√≥n clara: Instrucciones del sistema vs conversaci√≥n

Mejores pr√°cticas: Cu√°ndo usar cada tipo de template

Optimizaci√≥n: Aprovechamiento del formato nativo de chat de los LLMs



üèÅ Punto de partida

Tu c√≥digo actual usa PromptTemplate y deber√≠a verse as√≠:

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts import PromptTemplate
 
# ... configuraci√≥n de Streamlit ...
 
# Template actual con PromptTemplate
prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente √∫til y amigable llamado ChatBot Pro. 
 
Historial de conversaci√≥n:
{historial}
 
Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)
 
# Cadena actual
cadena = prompt_template | chat_model


üîÑ ¬øPor qu√© cambiar a ChatPromptTemplate?

Problemas con PromptTemplate:

‚ùå Todo mezclado: Instrucciones del sistema y conversaci√≥n en un solo bloque de texto
‚ùå Menos claro: Dif√≠cil separar qu√© es configuraci√≥n y qu√© es conversaci√≥n
‚ùå Menos natural: No aprovecha la estructura nativa de chat de los modelos
‚ùå Mantenimiento dif√≠cil: Cambios en las instrucciones requieren editar todo el template

Ventajas de ChatPromptTemplate:

‚úÖ Estructura clara: Separaci√≥n entre system, human y assistant messages
‚úÖ Mejor organizaci√≥n: Cada tipo de mensaje tiene su prop√≥sito espec√≠fico
‚úÖ M√°s natural: Aprovecha c√≥mo est√°n entrenados los modelos de chat
‚úÖ F√°cil mantenimiento: Cambios independientes en cada secci√≥n



üõ†Ô∏è Implementaci√≥n Paso a Paso

1. Actualizar Importaciones

Primer paso: A√±adir las importaciones necesarias.

# A√±ade esta importaci√≥n a las existentes
from langchain.prompts import ChatPromptTemplate


2. Crear el ChatPromptTemplate

Objetivo: Reemplazar el PromptTemplate actual con ChatPromptTemplate.

Tu turno: Reemplaza el template actual con este nuevo enfoque:

# Reemplaza el PromptTemplate existente con:
chat_prompt = ChatPromptTemplate.from_messages([
    # Mensaje del sistema - Define la personalidad una sola vez
    ("system", "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa."),
    
    # El historial y mensaje actual - se manejan como texto formateado
    ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
])


Diferencias clave:

("system", "..."): Define el comportamiento base del asistente (separado y claro)

("human", "..."): Contiene el historial y la pregunta actual

Estructura clara: Cada mensaje tiene un rol espec√≠fico



3. Actualizar la Cadena

Objetivo: Usar el nuevo template en la cadena.

# Actualiza la cadena (¬°sigue siendo simple!)
cadena = chat_prompt | chat_model


4. Personalizaci√≥n del Sistema (Desaf√≠o)

Objetivo: Hacer el mensaje del sistema configurable desde el sidebar.

with st.sidebar:
    st.header("Configuraci√≥n")
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Modelo", ["gpt-3.5-turbo", "gpt-4", "gpt-4o-mini"])
    
    # ¬°Nuevo! Personalidad configurable
    personalidad = st.selectbox(
        "Personalidad del Asistente",
        [
            "√ötil y amigable",
            "Profesional y formal", 
            "Casual y relajado",
            "Experto t√©cnico",
            "Creativo y divertido"
        ]
    )
    
    # Recrear modelo
    chat_model = ChatOpenAI(model=model_name, temperature=temperature)
    
    # Template din√°mico basado en personalidad
    system_messages = {
        "√ötil y amigable": "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa.",
        "Profesional y formal": "Eres un asistente profesional y formal. Proporciona respuestas precisas y bien estructuradas.",
        "Casual y relajado": "Eres un asistente casual y relajado. Habla de forma natural y amigable, como un buen amigo.",
        "Experto t√©cnico": "Eres un asistente experto t√©cnico. Proporciona respuestas detalladas con precisi√≥n t√©cnica.",
        "Creativo y divertido": "Eres un asistente creativo y divertido. Usa analog√≠as, ejemplos creativos y mant√©n un tono alegre."
    }
    
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", system_messages[personalidad]),
        ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
    ])
    
    cadena = chat_prompt | chat_model


üí° Conceptos Clave a Recordar

Estructura de ChatPromptTemplate

ChatPromptTemplate.from_messages([
    ("system", "Instrucciones base del asistente"),    # Configuraci√≥n
    ("human", "Contenido del usuario + historial"),   # Datos de entrada
    ("assistant", "Respuesta anterior (opcional)")    # Para few-shot examples
])


Roles de mensajes:

system: Instrucciones y configuraci√≥n del comportamiento

human: Mensajes del usuario (incluyendo contexto/historial)

assistant: Respuestas del modelo (para ejemplos o continuaci√≥n)

Ventajas del nuevo enfoque:

Claridad: Separaci√≥n visual entre configuraci√≥n y datos

Mantenimiento: Cambios independientes en cada secci√≥n

Flexibilidad: F√°cil intercambiar instrucciones del sistema

Mejores pr√°cticas: Sigue las convenciones de modelos de chat



üìù Soluci√≥n Completa

Aqu√≠ tienes el c√≥digo completo con la transformaci√≥n de PromptTemplate a ChatPromptTemplate, incluyendo el desaf√≠o del punto 4 (personalizaci√≥n del sistema):

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate
 
# Configuraci√≥n inicial
st.set_page_config(page_title="Chatbot B√°sico", page_icon="ü§ñ")
st.title("ü§ñ Chatbot B√°sico con LangChain")
st.markdown("Este es un *chatbot de ejemplo* construido con LangChain + Streamlit. ¬°Escribe tu mensaje abajo para comenzar!")
 
with st.sidebar:
    st.header("Configuraci√≥n")
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Modelo", ["gpt-3.5-turbo", "gpt-4", "gpt-4o-mini"])
    
    # PUNTO 4: Personalidad configurable
    personalidad = st.selectbox(
        "Personalidad del Asistente",
        [
            "√ötil y amigable",
            "Profesional y formal", 
            "Casual y relajado",
            "Experto t√©cnico",
            "Creativo y divertido"
        ]
    )
    
    # Recrear el modelo con nuevos par√°metros
    chat_model = ChatOpenAI(model=model_name, temperature=temperature)
    
    # Definir mensajes del sistema seg√∫n personalidad
    system_messages = {
        "√ötil y amigable": "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa.",
        "Profesional y formal": "Eres un asistente profesional y formal. Proporciona respuestas precisas y bien estructuradas.",
        "Casual y relajado": "Eres un asistente casual y relajado. Habla de forma natural y amigable, como un buen amigo.",
        "Experto t√©cnico": "Eres un asistente experto t√©cnico. Proporciona respuestas detalladas con precisi√≥n t√©cnica.",
        "Creativo y divertido": "Eres un asistente creativo y divertido. Usa analog√≠as, ejemplos creativos y mant√©n un tono alegre."
    }
    
    # NUEVO: ChatPromptTemplate con personalidad din√°mica
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", system_messages[personalidad]),
        ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
    ])
    
    # Crear cadena usando LCEL
    cadena = chat_prompt | chat_model
 
# Inicializar el historial de mensajes en session_state
if "mensajes" not in st.session_state:
    st.session_state.mensajes = []
 
# Renderizar historial existente
for msg in st.session_state.mensajes:
    if isinstance(msg, SystemMessage):
        continue  # no mostrar mensajes del sistema al usuario
    
    role = "assistant" if isinstance(msg, AIMessage) else "user"
    with st.chat_message(role):
        st.markdown(msg.content)
 
if st.button("üóëÔ∏è Nueva conversaci√≥n"):
    st.session_state.mensajes = []
    st.rerun()
 
# Input de usuario
pregunta = st.chat_input("Escribe tu mensaje:")
 
if pregunta:
    # Mostrar y almacenar mensaje del usuario
    with st.chat_message("user"):
        st.markdown(pregunta)
    
    # Preparar historial como texto
    historial_texto = ""
    for msg in st.session_state.mensajes[-10:]:  # √öltimos 10 mensajes
        if isinstance(msg, HumanMessage):
            historial_texto += f"Usuario: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            historial_texto += f"Asistente: {msg.content}\n"
    
    if not historial_texto:
        historial_texto = "(No hay historial previo)"
    
    # Generar y mostrar respuesta del asistente
    try:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
 
            # Streaming de la respuesta con ChatPromptTemplate
            for chunk in cadena.stream({"mensaje": pregunta, "historial": historial_texto}):
                full_response += chunk.content
                response_placeholder.markdown(full_response + "‚ñå")
            
            response_placeholder.markdown(full_response)
        
        # Almacenar mensajes en el historial
        st.session_state.mensajes.append(HumanMessage(content=pregunta))
        st.session_state.mensajes.append(AIMessage(content=full_response))
        
    except Exception as e:
        st.error(f"Error al generar respuesta: {str(e)}")
        st.info("Verifica que tu API Key de OpenAI est√© configurada correctamente.")


üîë Puntos Clave de la Soluci√≥n

1. La diferencia principal - PromptTemplate vs ChatPromptTemplate:

# ‚ùå ANTES: PromptTemplate (todo mezclado)
prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente √∫til y amigable llamado ChatBot Pro. 
 
Historial de conversaci√≥n:
{historial}
 
Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)
 
# ‚úÖ AHORA: ChatPromptTemplate (estructura clara)
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa."),
    ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
])


2. Ventajas implementadas:

‚úÖ Mejor estructura: System message separado del human message

‚úÖ M√°s claro: Instrucciones del asistente vs datos del usuario bien diferenciados

‚úÖ Mejor pr√°ctica: Sigue las convenciones de modelos de chat

‚úÖ F√°cil mantenimiento: Cambios independientes en system vs human messages



3. El uso sigue siendo id√©ntico:

# La cadena funciona exactamente igual
cadena = chat_prompt | chat_model
cadena.stream({"mensaje": pregunta, "historial": historial_texto})


4. Preparaci√≥n del historial:

# El historial se sigue formateando como texto
historial_texto = ""
for msg in st.session_state.mensajes[-10:]:
    if isinstance(msg, HumanMessage):
        historial_texto += f"Usuario: {msg.content}\n"
    elif isinstance(msg, AIMessage):
        historial_texto += f"Asistente: {msg.content}\n"


¬°La mejora es sutil pero importante: mejor organizaci√≥n y estructura m√°s profesional! üéâ